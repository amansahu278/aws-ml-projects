model_name: "distilgpt2"
output_dir: "checkpoints/distilgpt2"
train_subset: 100
eval_subset: 20
max_steps: 100 # Just proof of concept
per_device_train_batch_size: 1
per_device_eval_batch_size: 1
prompt_template: "alpaca"
learning_rate: 2.0e-4
warmup_steps: 20
logging_steps: 10
save_steps: 10
save_total_limit: 2
eval_steps: 10
seed: 1337
fp16: false
max_length: 64